{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Softmax regression\n",
    "\n",
    "In this exercise you will train a softmax regression model to recognize handwritten digits.\n",
    "  \n",
    "The general setup is as follows:\n",
    "* we are given a set of pairs $(x, y)$, where $x \\in R^D$ is a vector of real numbers representing the features, and $y \\in \\{1,...,c\\}$ is the target (in our case we have ten classes, so $c=10$),\n",
    "* for a given $x$ we model the probability of $y=j$ by $$h(x)_j=p_j = \\frac{e^{w_j^Tx}}{\\sum_{i=1}^c e^{w_i^Tx}},$$\n",
    "* to find the right $w$ we will optimize the so called multiclass log loss:\n",
    "$$L(y,p) = \\log{p_y},$$\n",
    "$$J(w) = -\\frac{1}{n}\\sum_{i=1}^n L(y_i,h(x)),$$\n",
    "* with the loss function in hand we can improve our guesses iteratively:\n",
    "    * $w_{ij}^{t+1} = w_{ij}^t - \\text{step_size} \\cdot \\frac{\\partial J(w)}{\\partial w_{ij}}$,\n",
    "* we can end the process after some predefined number of epochs (or when the changes are no longer meaningful)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's start with importing the MNIST dataset. For convenience, let's use Google's script from TensorFlow tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!wget -O MNIST_data.zip https://www.dropbox.com/sh/z7h50270eckbrd3/AAAmBulcP1UaEYBYyvBKqXSwa?dl=1\n",
    "!unzip MNIST_data.zip -d MNIST_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# needs tensorflow 1.0\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "train = mnist.train.next_batch(1000)\n",
    "test = mnist.train.next_batch(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's take a look at the data. Both train and test are tuples with two numpy array. In the first array you'll find the images (encoded as pixel intensities) and in the second one you'll find the labels (one-hot encoded)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print type(train)\n",
    "\n",
    "print type(train[0])\n",
    "print type(train[1])\n",
    "\n",
    "print train[0].shape\n",
    "print train[1].shape\n",
    "\n",
    "print train[0][:10]\n",
    "print train[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now let us see the data in a more humane way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "num_samples = 20\n",
    "samples = range(num_samples)\n",
    "fig, subplots = plt.subplots(1, num_samples)\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "for i, s in enumerate(subplots.flatten()):\n",
    "    s.imshow(np.reshape(train[0][i, :], [28, 28]), cmap='gray')\n",
    "    s.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Next, we prepare $X$ and $y$ variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = train[0]\n",
    "y = train[1]\n",
    "\n",
    "X_test = test[0]\n",
    "y_test = test[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "To train the model we will (obviously) use gradient descent. Inside the loop we need a method to compute the gradients. Let's start with implementing it, together with some helper functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# We will store the weights in a D x c matrix, where D is the number of features, and c is the number of classes\n",
    "#weights = (...) # TODO: Fill in, be sure to have the right shape!\n",
    "weights = np.zeros([X.shape[1], 10])\n",
    "\n",
    "\n",
    "def softmax(z):\n",
    "    ########################################\n",
    "    # TODO: implement the softmax function #\n",
    "    ########################################\n",
    "    return 0.\n",
    "\n",
    "def predict(weights, X):\n",
    "    ###################################\n",
    "    # TODO: compute the probabilities #\n",
    "    ###################################\n",
    "    return 0.\n",
    "\n",
    "def compute_loss_and_gradients(weights, X, y, l2_reg):\n",
    "    #############################################################################\n",
    "    # TODO: compute loss and gradients, don't forget to include regularization! #\n",
    "    #############################################################################\n",
    "    return loss, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We are now in position to complete the training pipeline.\n",
    "\n",
    "If you have problems with convergence, be sure to check the gradients numerically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "l2_reg = 0.5\n",
    "n_epochs = 250\n",
    "lr = 0.05\n",
    "\n",
    "losses = []\n",
    "for i in range(n_epochs):\n",
    "    loss, grad = compute_loss_and_gradients(weights, X, y, l2_reg)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    weights -= lr * grad\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now compute your accuracy on the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "##############################\n",
    "# TODO: compute the accuracy #\n",
    "##############################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also visualize the weights learned by our algorithm. Try to anticipate the result before executing the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig, subplots = plt.subplots(1, 10)\n",
    "fig.set_size_inches(15, 15)\n",
    "\n",
    "for i, s in enumerate(subplots.flatten()):\n",
    "    s.imshow(np.reshape(np.array(weights[:, i]), [28, 28]), cmap='gray')\n",
    "    s.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "Note that we only used a small portion of the data to develop the model. Now, implement the training on full data. \n",
    "Make sure to leverage the `mnist.train.next_batch(...)` method. Also, validate your model properly and find a good value for `l2_reg` hyperparameter. Try to experiment with `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "################################################\n",
    "# TODO: implement the proper training pipeline #\n",
    "################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
